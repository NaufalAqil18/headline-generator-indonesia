{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12022425,"sourceType":"datasetVersion","datasetId":7563852}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM,\n    get_linear_schedule_with_warmup\n)\nfrom tqdm import tqdm\nimport logging\nimport os\nimport matplotlib.pyplot as plt\nfrom rouge_score import rouge_scorer\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass NewsDataset(Dataset):\n    def __init__(self, articles, titles, tokenizer, max_article_length=512, max_title_length=128):\n        self.articles = articles\n        self.titles = titles\n        self.tokenizer = tokenizer\n        self.max_article_length = max_article_length\n        self.max_title_length = max_title_length\n\n    def __len__(self):\n        return len(self.articles)\n\n    def __getitem__(self, idx):\n        article = str(self.articles[idx])\n        title = str(self.titles[idx])\n\n        # Tokenize article\n        article_encoding = self.tokenizer(\n            article,\n            max_length=self.max_article_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        # Tokenize title\n        title_encoding = self.tokenizer(\n            title,\n            max_length=self.max_title_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': article_encoding['input_ids'].squeeze(),\n            'attention_mask': article_encoding['attention_mask'].squeeze(),\n            'labels': title_encoding['input_ids'].squeeze(),\n            'decoder_attention_mask': title_encoding['attention_mask'].squeeze()\n        }\n\nclass HeadlineGenerator:\n    def __init__(self, model_name=\"cahya/bert2bert-indonesian-summarization\", \n                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        self.model.to(device)\n        \n        # Create directories for saving artifacts\n        os.makedirs('models', exist_ok=True)\n        os.makedirs('plots', exist_ok=True)\n        \n        # Initialize metrics tracking\n        self.train_losses = []\n        self.val_losses = []\n        self.rouge_scores = []\n        \n        # Initialize ROUGE scorer\n        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n    def prepare_data(self, df, batch_size=8, test_size=0.1, val_size=0.1, random_state=42):\n        \"\"\"Prepare train, validation, and test datasets\"\"\"\n        # First split: training + validation vs test\n        train_val_df, test_df = train_test_split(\n            df, test_size=test_size, random_state=random_state\n        )\n        \n        # Second split: training vs validation\n        train_df, val_df = train_test_split(\n            train_val_df, \n            test_size=val_size/(1-test_size),\n            random_state=random_state\n        )\n        \n        logger.info(f\"Dataset sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n        \n        # Create datasets\n        train_dataset = NewsDataset(\n            train_df['content'].values,\n            train_df['judul'].values,\n            self.tokenizer\n        )\n        val_dataset = NewsDataset(\n            val_df['content'].values,\n            val_df['judul'].values,\n            self.tokenizer\n        )\n        test_dataset = NewsDataset(\n            test_df['content'].values,\n            test_df['judul'].values,\n            self.tokenizer\n        )\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n        \n        return train_loader, val_loader, test_loader\n\n    def compute_rouge_scores(self, predictions, references):\n        \"\"\"Compute ROUGE scores between predictions and references\"\"\"\n        scores = {\n            'rouge1': {'precision': 0, 'recall': 0, 'fmeasure': 0},\n            'rouge2': {'precision': 0, 'recall': 0, 'fmeasure': 0},\n            'rougeL': {'precision': 0, 'recall': 0, 'fmeasure': 0}\n        }\n        \n        for pred, ref in zip(predictions, references):\n            score = self.rouge_scorer.score(ref, pred)\n            for metric in scores.keys():\n                scores[metric]['precision'] += score[metric].precision\n                scores[metric]['recall'] += score[metric].recall\n                scores[metric]['fmeasure'] += score[metric].fmeasure\n        \n        # Average scores\n        n = len(predictions)\n        for metric in scores.keys():\n            for key in scores[metric].keys():\n                scores[metric][key] /= n\n        \n        return scores\n\n    def train_epoch(self, train_loader, optimizer, scheduler, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n        \n        for batch in progress_bar:\n            # Move batch to device\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            decoder_attention_mask = batch['decoder_attention_mask'].to(self.device)\n            \n            # Forward pass\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n                decoder_attention_mask=decoder_attention_mask\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            # Update progress bar\n            progress_bar.set_postfix({'loss': loss.item()})\n        \n        avg_loss = total_loss / len(train_loader)\n        self.train_losses.append(avg_loss)\n        return avg_loss\n\n    def validate(self, val_loader, epoch):\n        \"\"\"Validate the model\"\"\"\n        self.model.eval()\n        total_loss = 0\n        all_predictions = []\n        all_references = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f'Validating Epoch {epoch}'):\n                # Move batch to device\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                decoder_attention_mask = batch['decoder_attention_mask'].to(self.device)\n                \n                # Forward pass\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    decoder_attention_mask=decoder_attention_mask\n                )\n                \n                loss = outputs.loss\n                total_loss += loss.item()\n                \n                # Generate predictions\n                predictions = self.model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=128,\n                    num_beams=4,\n                    early_stopping=True\n                )\n                \n                # Decode predictions and references\n                decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n                decoded_refs = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n                \n                all_predictions.extend(decoded_preds)\n                all_references.extend(decoded_refs)\n        \n        # Calculate metrics\n        avg_loss = total_loss / len(val_loader)\n        self.val_losses.append(avg_loss)\n        \n        rouge_scores = self.compute_rouge_scores(all_predictions, all_references)\n        self.rouge_scores.append(rouge_scores)\n        \n        return avg_loss, rouge_scores\n\n    def plot_training_progress(self):\n        \"\"\"Plot training metrics\"\"\"\n        plt.figure(figsize=(15, 5))\n        \n        # Plot losses\n        plt.subplot(1, 2, 1)\n        plt.plot(self.train_losses, label='Train Loss')\n        plt.plot(self.val_losses, label='Validation Loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        # Plot ROUGE scores\n        plt.subplot(1, 2, 2)\n        epochs = range(1, len(self.rouge_scores) + 1)\n        \n        plt.plot(epochs, [s['rouge1']['fmeasure'] for s in self.rouge_scores], label='ROUGE-1')\n        plt.plot(epochs, [s['rouge2']['fmeasure'] for s in self.rouge_scores], label='ROUGE-2')\n        plt.plot(epochs, [s['rougeL']['fmeasure'] for s in self.rouge_scores], label='ROUGE-L')\n        \n        plt.title('ROUGE Scores')\n        plt.xlabel('Epoch')\n        plt.ylabel('F1 Score')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig('plots/training_progress.png')\n        plt.close()\n\n    def train(self, train_loader, val_loader, epochs=3, learning_rate=2e-5):\n        \"\"\"Training loop with monitoring\"\"\"\n        logger.info(f\"\\nStarting training on {self.device}\")\n        logger.info(f\"Number of epochs: {epochs}\")\n        logger.info(f\"Learning rate: {learning_rate}\")\n        logger.info(f\"Training batches: {len(train_loader)}\")\n        logger.info(f\"Validation batches: {len(val_loader)}\")\n        \n        # Setup optimizer and scheduler\n        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n        total_steps = len(train_loader) * epochs\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=total_steps\n        )\n        \n        best_val_loss = float('inf')\n        for epoch in range(epochs):\n            logger.info(f\"\\nEpoch {epoch + 1}/{epochs}\")\n            \n            # Training phase\n            train_loss = self.train_epoch(train_loader, optimizer, scheduler, epoch + 1)\n            logger.info(f\"Average training loss: {train_loss:.4f}\")\n            \n            # Validation phase\n            val_loss, rouge_scores = self.validate(val_loader, epoch + 1)\n            logger.info(f\"Validation loss: {val_loss:.4f}\")\n            \n            # Log ROUGE scores\n            logger.info(\"\\nROUGE Scores:\")\n            for metric, scores in rouge_scores.items():\n                logger.info(f\"{metric}: {scores['fmeasure']:.4f}\")\n            \n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                self.save_model(f'models/best_model_epoch_{epoch + 1}')\n                logger.info(f\"New best model saved!\")\n            \n            # Plot progress\n            self.plot_training_progress()\n            \n            # Early stopping check\n            if len(self.val_losses) > 2 and self.val_losses[-1] > self.val_losses[-2]:\n                logger.warning(\"Validation loss increased. Consider early stopping.\")\n        \n        logger.info(\"\\nTraining completed!\")\n        logger.info(f\"Best validation loss: {best_val_loss:.4f}\")\n\n    def save_model(self, path):\n        \"\"\"Save model and tokenizer\"\"\"\n        self.model.save_pretrained(path)\n        self.tokenizer.save_pretrained(path)\n        logger.info(f\"Model saved to {path}\")\n\n    def load_model(self, path):\n        \"\"\"Load model and tokenizer\"\"\"\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(path)\n        self.tokenizer = AutoTokenizer.from_pretrained(path)\n        self.model.to(self.device)\n        logger.info(f\"Model loaded from {path}\")\n\n    def generate_headline(self, article_text):\n        \"\"\"Generate a headline for a given article\"\"\"\n        self.model.eval()\n        \n        # Tokenize input\n        inputs = self.tokenizer(\n            article_text,\n            max_length=512,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(self.device)\n        \n        # Generate headline\n        with torch.no_grad():\n            outputs = self.model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n        \n        # Decode and return headline\n        headline = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return headline\n\ndef main():\n    # Example usage\n    model = HeadlineGenerator()\n    \n    # Load your dataset\n    df = pd.read_csv('/kaggle/input/dataset-kompas/kompas_all_articles.csv')  # Replace with your dataset path\n    \n    # Prepare data\n    train_loader, val_loader, test_loader = model.prepare_data(df)\n    \n    # Train model\n    model.train(train_loader, val_loader, epochs=3)\n    \n    # Example headline generation\n    article = \"\"\"\n    Pemerintah Indonesia resmi mengumumkan peluncuran program baru yang bertujuan untuk meningkatkan literasi digital di kalangan pelajar. \n    Program ini akan dilaksanakan di lebih dari 500 sekolah di seluruh provinsi mulai bulan depan, dengan pelatihan bagi guru dan penyediaan perangkat teknologi.\n    \"\"\"\n    \n    headline = model.generate_headline(article)\n    print(f\"\\nGenerated headline: {headline}\")\n\nif __name__ == \"__main__\":\n    main() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:04:32.636323Z","iopub.execute_input":"2025-06-01T11:04:32.637017Z","iopub.status.idle":"2025-06-01T12:13:14.584708Z","shell.execute_reply.started":"2025-06-01T11:04:32.636997Z","shell.execute_reply":"2025-06-01T12:13:14.584105Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.bert.modeling_bert.BertModel'> is overwritten by shared encoder config: BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nConfig of the decoder: <class 'transformers.models.bert.modeling_bert.BertLMHeadModel'> is overwritten by shared decoder config: BertConfig {\n  \"add_cross_attention\": true,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": true,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nTraining Epoch 1:   0%|          | 0/1332 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nTraining Epoch 1: 100%|██████████| 1332/1332 [19:20<00:00,  1.15it/s, loss=0.3]  \nValidating Epoch 1:  10%|▉         | 16/167 [00:20<03:09,  1.25s/it]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nValidating Epoch 1: 100%|██████████| 167/167 [03:30<00:00,  1.26s/it]\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 40, 'min_length': 20, 'early_stopping': True, 'num_beams': 10, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nTraining Epoch 2:   0%|          | 0/1332 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nTraining Epoch 2: 100%|██████████| 1332/1332 [19:19<00:00,  1.15it/s, loss=0.213]\nValidating Epoch 2: 100%|██████████| 167/167 [03:32<00:00,  1.27s/it]\nTraining Epoch 3:   0%|          | 0/1332 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nTraining Epoch 3: 100%|██████████| 1332/1332 [19:20<00:00,  1.15it/s, loss=0.257] \nValidating Epoch 3: 100%|██████████| 167/167 [03:28<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated headline: pemerintah resmikan 500 sekolah di seluruh provinsi mulai bulan depan, ini syarat dan cara mereka masukkan\n","output_type":"stream"}],"execution_count":6}]}